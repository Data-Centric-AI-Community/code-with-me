{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-q8JKF54L-p",
        "outputId": "7e4f9423-916d-4084-b668-3e0a9feff0ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.7.10.post1-py3-none-any.whl (604 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.6/604.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting milvus\n",
            "  Downloading milvus-2.2.11-py3-none-manylinux2014_x86_64.whl (51.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymilvus\n",
            "  Downloading pymilvus-2.2.13-py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.236-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from llama-index)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama-index)\n",
            "  Downloading dataclasses_json-0.5.12-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.26.16)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.11.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: grpcio<=1.56.0,>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.56.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n",
            "Collecting environs<=9.5.0 (from pymilvus)\n",
            "  Downloading environs-9.5.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting ujson>=2.0.0 (from pymilvus)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting langsmith<0.0.11,>=0.0.10 (from langchain)\n",
            "  Downloading langsmith-0.0.10-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama-index) (2.4.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Installing collected packages: ujson, python-dotenv, mypy-extensions, milvus, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, langsmith, environs, pymilvus, openai, dataclasses-json, langchain, llama-index\n",
            "Successfully installed dataclasses-json-0.5.12 environs-9.5.0 langchain-0.0.236 langsmith-0.0.10 llama-index-0.7.10.post1 marshmallow-3.19.0 milvus-2.2.11 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pymilvus-2.2.13 python-dotenv-1.0.0 tiktoken-0.4.0 typing-inspect-0.9.0 ujson-5.8.0\n"
          ]
        }
      ],
      "source": [
        "! pip install llama-index nltk milvus pymilvus langchain python-dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFAdLV4J4L-q",
        "outputId": "223080ed-afb3-43f2-f6a6-940bac2c2a82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/yujiantang/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBU182o64L-r"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    GPTVectorStoreIndex,\n",
        "    GPTSimpleKeywordTableIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    LLMPredictor,\n",
        "    ServiceContext,\n",
        "    StorageContext\n",
        ")\n",
        "from langchain.llms.openai import OpenAIChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSWBWTIg4L-r",
        "outputId": "dbf8154b-c255-456d-8043-d815d2ddc1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "    __  _________ _   ____  ______\n",
            "   /  |/  /  _/ /| | / / / / / __/\n",
            "  / /|_/ // // /_| |/ / /_/ /\\ \\\n",
            " /_/  /_/___/____/___/\\____/___/ {Lite}\n",
            "\n",
            " Welcome to use Milvus!\n",
            "\n",
            " Version:   v2.2.8-lite\n",
            " Process:   49630\n",
            " Started:   2023-05-18 16:00:02\n",
            " Config:    /Users/yujiantang/.milvus.io/milvus-server/2.2.8/configs/milvus.yaml\n",
            " Logs:      /Users/yujiantang/.milvus.io/milvus-server/2.2.8/logs\n",
            "\n",
            " Ctrl+C to exit ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[93m[has_collection] retry:4, cost: 0.27s, reason: <_MultiThreadedRendezvous: StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet. please wait>\u001b[0m\n",
            "\u001b[93m[has_collection] retry:5, cost: 0.81s, reason: <_MultiThreadedRendezvous: StatusCode.UNAVAILABLE, internal: Milvus Proxy is not ready yet. please wait>\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from llama_index.vector_stores import MilvusVectorStore\n",
        "from milvus import default_server\n",
        "\n",
        "default_server.start()\n",
        "vector_store = MilvusVectorStore(\n",
        "   host = \"127.0.0.1\",\n",
        "   port = default_server.listen_port\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyzZQx9Z4L-r"
      },
      "outputs": [],
      "source": [
        "wiki_titles = [\"Toronto\", \"Seattle\", \"San Francisco\", \"Chicago\", \"Boston\", \"Washington, D.C.\", \"Cambridge, Massachusetts\", \"Houston\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px3yEJFy4L-s"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "for title in wiki_titles:\n",
        "    response = requests.get(\n",
        "        'https://en.wikipedia.org/w/api.php',\n",
        "        params={\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': title,\n",
        "            'prop': 'extracts',\n",
        "            # 'exintro': True,\n",
        "            'explaintext': True,\n",
        "        }\n",
        "    ).json()\n",
        "    page = next(iter(response['query']['pages'].values()))\n",
        "    wiki_text = page['extract']\n",
        "\n",
        "    data_path = Path('data')\n",
        "    if not data_path.exists():\n",
        "        Path.mkdir(data_path)\n",
        "\n",
        "    with open(data_path / f\"{title}.txt\", 'w') as fp:\n",
        "        fp.write(wiki_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDsmwlUB4L-s"
      },
      "outputs": [],
      "source": [
        "# Load all wiki documents\n",
        "city_docs = {}\n",
        "for wiki_title in wiki_titles:\n",
        "    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaWzMyoi4L-s",
        "outputId": "864387be-f424-4546-b969-fff89c688ea0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yujiantang/Documents/workspace/hello_world_project/hw_milvus/lib/python3.10/site-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm_predictor_chatgpt = LLMPredictor(llm=OpenAIChat(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zOxd3O74L-t"
      },
      "outputs": [],
      "source": [
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jbhmd-N4L-t"
      },
      "outputs": [],
      "source": [
        "# Build city document index\n",
        "city_indices = {}\n",
        "index_summaries = {}\n",
        "for wiki_title in wiki_titles:\n",
        "    city_indices[wiki_title] = GPTVectorStoreIndex.from_documents(city_docs[wiki_title], service_context=service_context, storage_context=storage_context)\n",
        "    # set summary text for city\n",
        "    index_summaries[wiki_title] = f\"Wikipedia articles about {wiki_title}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqUpiCfS4L-t"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.composability import ComposableGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBvmGTbb4L-t"
      },
      "outputs": [],
      "source": [
        "graph = ComposableGraph.from_indices(\n",
        "    GPTSimpleKeywordTableIndex,\n",
        "    [index for _, index in city_indices.items()],\n",
        "    [summary for _, summary in index_summaries.items()],\n",
        "    max_keywords_per_chunk=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvUuOrN-4L-u"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
        "decompose_transform = DecomposeQueryTransform(\n",
        "    llm_predictor_chatgpt, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0EfRhHu4L-u"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
        "custom_query_engines = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV8VBVCD4L-u"
      },
      "outputs": [],
      "source": [
        "for index in city_indices.values():\n",
        "    query_engine = index.as_query_engine(service_context=service_context)\n",
        "    transform_extra_info = {'index_summary': index.index_struct.summary}\n",
        "    tranformed_query_engine = TransformQueryEngine(query_engine, decompose_transform, transform_extra_info=transform_extra_info)\n",
        "    custom_query_engines[index.index_id] = tranformed_query_engine\n",
        "\n",
        "custom_query_engines[graph.root_index.index_id] = graph.root_index.as_query_engine(\n",
        "    retriever_mode='simple',\n",
        "    response_mode='tree_summarize',\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine_decompose = graph.as_query_engine(\n",
        "    custom_query_engines=custom_query_engines,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjbPohFL4L-u",
        "outputId": "11a2fdd8-5dbe-42d0-acaf-e5b1320b2d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What is the name of the airport in Seattle?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What is the name of the airport in Seattle?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are the major airports in Houston?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are the major airports in Houston?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable features of the Toronto airport?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the airports in Seattle, Houston, and Toronto. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable features of the Toronto Pearson International Airport?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_chatgpt = query_engine_decompose.query(\n",
        "    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okttuVsk4L-u",
        "outputId": "c9e50277-4c41-4770-ecf2-13a6393d00a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seattle has one major airport called Seattle-Tacoma International Airport, while Houston has two major airports called George Bush Intercontinental Airport and William P. Hobby Airport, as well as a third municipal airport called Ellington Airport. Toronto's busiest airport is called Toronto Pearson International Airport and is located on the city's western boundary with Mississauga. It offers limited commercial and passenger service to nearby destinations in Canada and the United States. Seattle-Tacoma International Airport and George Bush Intercontinental Airport are both major international airports, while William P. Hobby Airport and Ellington Airport are smaller and serve more regional destinations. Toronto Pearson International Airport is Canada's busiest airport and offers a direct link to Union Station through the Union Pearson Express train service.\n"
          ]
        }
      ],
      "source": [
        "print(str(response_chatgpt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or55xQTo4L-u"
      },
      "outputs": [],
      "source": [
        "custom_query_engines = {}\n",
        "for index in city_indices.values():\n",
        "    query_engine = index.as_query_engine(service_context=service_context)\n",
        "    custom_query_engines[index.index_id] = query_engine\n",
        "\n",
        "custom_query_engines[graph.root_index.index_id] = graph.root_index.as_query_engine(\n",
        "    retriever_mode='simple',\n",
        "    response_mode='tree_summarize',\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine = graph.as_query_engine(\n",
        "    custom_query_engines=custom_query_engines,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVxa8vxe4L-v",
        "outputId": "def56073-4e10-4f2f-924d-0efe26b2fc3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The context information provided does not contain enough information to answer the question.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_chatgpt = query_engine.query(\n",
        "    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n",
        ")\n",
        "str(response_chatgpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "966NfOUT4L-v",
        "outputId": "35cd4ae9-2604-4340-ce0c-1f676f478166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the sports environment of Houston and Boston. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What sports teams are based in Houston?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the sports environment of Houston and Boston. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What sports teams are based in Houston?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the sports environment of Houston and Boston. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable sports teams based in Boston?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the sports environment of Houston and Boston. \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable sports teams based in Boston?\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_chatgpt = query_engine_decompose.query(\n",
        "    \"Compare and contrast the sports environment of Houston and Boston. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePOKvJ-g4L-v",
        "outputId": "39b0d7d4-8bb5-40ee-83de-6f965f5ff444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Houston has sports teams for every major professional league except the National Hockey League, while Boston has teams for the NFL, MLB, NBA, and NHL. Both cities have professional soccer teams, with Houston having a Major League Soccer franchise and Boston having a National Women's Soccer League team. Boston also has several college sports teams and Esports teams, while Houston does not have any notable college sports teams or Esports teams. Both cities are known for hosting major sporting events, with Boston hosting the Boston Marathon and the Head of the Charles Regatta, while Houston does not have any comparable events. Overall, Boston has a more diverse sports environment with teams in all major professional leagues and a strong presence in college sports and Esports.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(response_chatgpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltSFHBnh4L-v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hw_milvus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}